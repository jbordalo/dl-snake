Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. If in doubt you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

Atenção:
- Não edite este ficheiro em programas como Word e afins. Use exclusivamente um editor de texto simples. Em caso de dúvida, use o editor do Spyder.
- Não altere a estrutura deste ficheiro. Preencha as respostas apenas nos espaços respectivos (a seguir à tag R#:)
- Pode adicionar linhas no espaço para as respostas mas as respostas devem ser sucintas e directas.

Linking images and reports/Incluir imagens e relatórios
- You can link a .png or .html file in your answers by typing the name of the file in a separate line. The file must be in the same folder as this TP1.txt file. See the examples below:
- Pode ligar às respostas um ficheiro .png ou .html escrevendo o nome do ficheiro numa linha separada. O ficheiro tem de estar presente na mesma pasta em que está este ficheiro TP1.txt. Por exemplo:

exemplo.png
exemplo.html

PERGUNTAS/QUESTIONS:

Q1: Explain and justify the architecture of your agent, describing the network, input and output, and what experiments you did to choose this particular architecture.
Q1; Explique e justifique a arquitectura do seu agente, descrevendo a rede, os valores de entrada e saída e que experiências fez para escolher esta rede em particular.
R1: Utilizámos uma rede que recebe como input o frame atual do mapa do jogo e consiste numa secção inicial de camadas de convolução para extrair features relevantes deste mesmo para serem fornecidas à secção seguinte da rede, que consiste em várias camadas densas com ativação relu. A rede termina com uma camada de ativação linear dado que a aproximação da Q Table consiste num problema de regressão. Escolhemos esta rede pois camadas de convolução são úteis para extrair features de imagens e a secção densa conseguirá aprender a função Q com base nestas. É de notar que não utilizámos pooling para perservar os detalhes da informação original.
Inicialmente pensamos em utilizar a rede do primeiro trabalho mas notámos que utilizar uma camada de GlobalAveragePooling2D na última secção tinha um efeito negativo na capacidade da rede em reduzir a loss, provavelmente devido ao facto de esta mesma perder a informação espacial da camada antecedente. Ao substituir a mesma com um conjunto de camadas densas conseguimos que a rede se ajustasse melhor à Q Table prevista. Mais ainda, também se observou que as camadas de pooling na secção de convolução tinham um efeito negativo na redução da loss e conseguimos também melhorar os resultados da mesma adicionando mais filtros de convolução.


Q2: Describe the scenario for which you tried to optimize your agent, justifying your choice and describing the experiments you did to select this scenario.
Q2: Descreva o cenário para o qual tentou optimizar o seu agente, justificando a sua escolha e descrevendo que experiências fez para seleccionar este cenário.
R2: Inicialmente o objetivo era colocar o agente a comer consistentemente algumas maçãs mas não o conseguimos atingir. Como estávamos cientes do problema das rare rewards neste jogo, tentámos otimizar o agente numa grelha 14x14 para tentar minimizar este problema. Acabámos por conseguir que este sobrevivesse por um período relativamente longo (100-200 steps) com alguma consistência explorando uma parte significativa do mapa mas não mais do que isso. Em retrospetiva, para continuar a utilizar estes algoritmos, deveríamos utilizar grelhas de dimensão ainda mais reduzida, de modo a forçar a cobra a colidir mais frequentemente com as maçãs de modo natural para facilitar esta aprendizagem.


Q3: Explain how you generate the experiences for training your agent. Do not forget to explain the details like the balance between exploitation and exploration, how experiences are selected, what information you store in each experience, how large is the pool of experiences and how frequently are new experiences generated. Justify your choices.
Q3: Explique como gerou as experiências para treinar seu agente. Não se esqueça de explicar os detalhes como o equilíbrio entre exploração aleatória e guiada (exploration e exploitation), como as experiências são selecionadas, que informação guarda em cada experiência, quão grande é o conjunto de experiências e com que frequência novas experiências são geradas. Justifique estas escolhas.
R3: Para auxiliar o agente a aprender mais rapidamente, inicialmente gerámos experiências recorrendo a um agente greedy, que escolhia sempre a ação que o colocasse à menor distância de manhattan da maçã. Cada uma destas experiências foi armazenada no formato (estado inicial, ação, recompensa, estado seguinte, terminalidade do estado seguinte).
Depois, utilizámos decaying epsilon-greedy exploration para gerar mais experiências aleatórias com as quais o agente pudesse aprender ao longo do tempo para tentar assegurar uma cobertura do espaço de soluções menos enviesada pelos exemplos greedy. Assim, em cada momento de treino era selecionado um conjunto aleatório de experiências para este mesmo fim de modo a assegurar que as experiências greedy (mais bem sucedidas em comer maçãs) não acabassem "esquecidas".
A um certo ponto também experimentámos gerar experiências greedy aquando da iteração para um próximo jogo mas verificámos que estávamos a inundar o nosso agente com um tipo de experiência muito particular (o agente greedy tendia a só comer as maçãs enquanto seguia em frente) e optámos por apenas gerar as experiências greedy ao início e fazendo uso de uma memória de grandes dimensões (50000) para assegurar que estes não seriam esquecidos tão depressa (poderíamos também ter alterado a sua frequência de geração).


Q4: Explain how you trained your agent: the algorithm used, the discount factor (gamma), the schedule for generating new experiences and training the agent and other techniques. Justify your choices and explain what experiments you did and which alternatives you tried.
Q4: Explique como treinou o seu agente: o algoritmo utilizado, o factor de desconto (gama), a coordenação da geração de novas experiências e treinao do agente, e outras técnicas que tenha usado. Justifique suas escolhas e explique que alternativas testou e como validou experimentalmente estas escolhas.
R4: Utilizámos deep Q learning para treinar o agente com o fator de desconto a 0.95, com o objetivo de o incentivar a dirigir-se às maçãs. Para gerar experiências acabámos por memorizar as escolhas do agente juntamente com escolhas aleatórias em proporção a epsilon. Também utilizámos duas redes, uma sempre treinada, e outra com a função de prever os valores da Q Table. Esta última é só atualizada com os pesos da primeira periodicamente de modo a favorecer a estabilidade do algoritmo (ao início tivemos o problema de uma loss crescente, aumentar este período ajudou a combater o problema mas acabámos por optar em simplesmente reduzir o learning rate).
Porém, mesmo com o fator de desconto a 0.95, o agente continuou a não se dirigir às maçãs, provavelmente devido ao raro contacto com as mesmas, mesmo com as experiências iniciais que lhe demos. Aumentar o número de maçãs para ajudar o agente a comê-las acidentalmente não ajudou muito (a nossa heurística greedy também não estava preparada para este cenário). Adicionar relva fez com que o agente explorasse mais o terreno e sobrevivesse por mais tempo mas continuou a não contribuir para que o mesmo se dirigisse às maçãs. Tentámos ainda forçar o agente a entender que quando se aproximasse de uma maçã lhe seria benéfico seguir em sua direção ao dar-lhe experiências dos últimos 3 steps antes do agente greedy as comer mas o agente pareceu aprender simplesmente que andar em frente seria quase sempre a melhor opção, caminhando em direção às paredes. Tentámos também corrigir este cenário dando-lhe experiências dos maus resultados de ir em direção à parede mas todas estas intervenções acabaram por causar piores resultados do que deixar o agente explorar autonomamente.

Q5: Discuss this problem and your solution, describing the agent's performance (score, survival, etc.), which aspects of the game were most difficult for the agent to learn, how you tried to remedy these difficulties and what ideas you might have tried if you had more time.
Q5: Discuta este problema e a sua solução, descrevendo o desempenho do agente (pontuação, sobrevivência, etc), que aspectos do jogo foram mais difíceis de aprender para o agente, como tentou colmatar essas dificuldades e que ideias ficaram que poderia experimentar se tivesse mais tempo.
R5: O jogo do snake em grelhas grandes sofre do problema de ter rare rewards, sendo bastante difícil para o agente descobrir o verdadeiro objetivo do jogo com movimentos aleatórios. Assim, como o agente se suicida muito mais frequentemente do que come uma maçã, este acaba por pensar que o objetivo do jogo é simplesmente sobreviver (mais ainda com relva, passando o objetivo também a ser comer relva). Deste modo, acabámos por efetivamente observar que o nosso agente conseguiu apenas aprender a sobreviver (100-200 steps consistentemente, ocasionalmente 400 no máximo), apenas comendo uma maçã raramente. Tentámos combater este problema gerando mais experiências em que uma maçã fosse comida por via de um agente greedy. No entanto, esta adição não acabou por ser suficiente e a cobra começou a sobrevalorizar a ação de seguir em frente já que ao otimizar a distância de manhattan esta passou a ser a ação mais frequente e que acabava por normalmente levar à maçã. Por esta razão só utilizámos estes exemplos no começo do treino, deixando a cobra aprender cada vez mais com as suas experiências e outras aleatórias ao longo do tempo. Para mitigar este problema também poderíamos ter experiementado misturar uma heurística que fizesse uso da distância euclidiana, já que esta tenderia a fornecer mais movimentos na diagonal, equilibrando melhor as ações realizadas. Adicionar exemplos de estratégias ótimas com recurso a pesquisa em largura ou A star também poderia ser útil para gerar mais experiências de médio prazo no jogo. Finalmente, para contribuir para uma melhor aprendizagem das melhores estratégias para o final do jogo poderíamos utilizar agentes que percorressem circuitos hamiltonianos no mapa.
Outro fator limitante da rede atual é o facto de esta não ter acesso a informação temporal, existindo muitas situações em que com só um frame é impossível determinar qual a direção atual da cobra, o que impossibilita a escolha de uma jogada apropriada em muitas situações. Isto poderia ser corrigido fornecendo à rede também o frame anterior ou até um conjunto destes caso se relevasse vantajoso. No entanto, devido à potencial complexidade adicional para treinar a rede neste cenário, uma solução mais adequada seria (caso não se considere que esta evade de alguma o problema original) tentar aprender ações que influenciassem a  alteração da direção da cobra de modo absoluto em vez de relativo, como é o caso em muitas implementações do jogo. Desde modo a informação temporal deixaria de ser relevante e não estimamos que o facto de passarem, para cada instante, a existir duas ações equivalentes fosse assim tão problemático (seria impossível inverter o sentido da cobra, podendo ser esta ação sempre interpretada como manter o curso atual).
Outra abordagem seria ainda abandonar por completo o deep Q learning e investigar outras alternativas, como Actor Critic, que aparentemente da nossa leitura superficial é menos intensivo na exigência da recolha de exemplos, que acaba por ser um dos campos em que temos atualmente problemas.
